{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Gold Prices with PCA and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates and improves a linear regression model using gold and stock index prices from 2012 until 2019 --[https://www.kaggle.com/sid321axn/gold-price-prediction-dataset](http://) . The data has 81 features. The index consists of individual days. The target feature is \"adjusted close\". We are trying to predict what the closing price of gold will be given predictors of the gold market itself as well as other stock market indexes. The regular \"Close\" feature is also removed during model construction. That leaves 78 features to predict the adjusted close price of gold.\n",
    "\n",
    "I started out building a simple linear regression model with predictors I imagined would be good to start with: the daily high, daily low, daily open, and volume being traded. From there, I created a gradient boosting model and compared its performance with the linear regression model. In this case, the linear regression model had a better mean absolute error than gradient boosting, even when put through a 10-fold cross validation.\n",
    "\n",
    "Then, I went into feature engineering for a more detailed look of the data. I put 78 features through mutual information analysis. All of the predictors I used for the earlier linear regression and gradient boosting models were the highest performers, with the exception of volume. Next, I created a custom feature that measured daily volatility of gold prices. I compared this with the adjusted close price over time. What I found is that daily volatility was slightly correlated with adjusted close price.\n",
    "\n",
    "Finally, I continue feature engineering by using PCA to take the top six features with the highest mutual information to adjusted close and created six principal components. I used those six principal components in a new linear regression pipeline. This final model was the best performing of this notebook. See the bottom for the final results.\n",
    "\n",
    "Thanks to *Manu Siddhartha* for the dataset! Check out his profile here. [https://www.kaggle.com/sid321axn](http://)\n",
    "\n",
    "All feedback is welcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-09T08:53:39.152382Z",
     "iopub.status.busy": "2022-11-09T08:53:39.151814Z",
     "iopub.status.idle": "2022-11-09T08:53:40.780850Z",
     "shell.execute_reply": "2022-11-09T08:53:40.779319Z",
     "shell.execute_reply.started": "2022-11-09T08:53:39.152273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-09T08:54:45.194854Z",
     "iopub.status.busy": "2022-11-09T08:54:45.194095Z",
     "iopub.status.idle": "2022-11-09T08:54:45.307774Z",
     "shell.execute_reply": "2022-11-09T08:54:45.306597Z",
     "shell.execute_reply.started": "2022-11-09T08:54:45.194807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.740005</td>\n",
       "      <td>154.949997</td>\n",
       "      <td>151.710007</td>\n",
       "      <td>21521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154.309998</td>\n",
       "      <td>155.369995</td>\n",
       "      <td>153.899994</td>\n",
       "      <td>18124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155.479996</td>\n",
       "      <td>155.860001</td>\n",
       "      <td>154.360001</td>\n",
       "      <td>12547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.820007</td>\n",
       "      <td>157.429993</td>\n",
       "      <td>156.580002</td>\n",
       "      <td>9136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156.979996</td>\n",
       "      <td>157.529999</td>\n",
       "      <td>156.130005</td>\n",
       "      <td>11996100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open        High         Low    Volume\n",
       "0  154.740005  154.949997  151.710007  21521900\n",
       "1  154.309998  155.369995  153.899994  18124300\n",
       "2  155.479996  155.860001  154.360001  12547200\n",
       "3  156.820007  157.429993  156.580002   9136300\n",
       "4  156.979996  157.529999  156.130005  11996100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save filepath for easier access\n",
    "gold_file_path = 'FINAL_USO.csv'\n",
    "\n",
    "# Read the data with pandas and store it in a dataframe titled gold data\n",
    "df = pd.read_csv(gold_file_path)\n",
    "\n",
    "y = df['Adj Close'] \n",
    "\n",
    "# We will start out by selecting features gold ETF features\n",
    "gold_features = ['Open','High', 'Low', 'Volume']\n",
    "X = df[gold_features]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check to see if we have any missing values that we need to impute or remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:12.695485Z",
     "iopub.status.busy": "2021-08-03T19:37:12.695187Z",
     "iopub.status.idle": "2021-08-03T19:37:12.701366Z",
     "shell.execute_reply": "2021-08-03T19:37:12.700738Z",
     "shell.execute_reply.started": "2021-08-03T19:37:12.695458Z"
    }
   },
   "outputs": [],
   "source": [
    "# There are no null values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:12.703271Z",
     "iopub.status.busy": "2021-08-03T19:37:12.702875Z",
     "iopub.status.idle": "2021-08-03T19:37:12.729003Z",
     "shell.execute_reply": "2021-08-03T19:37:12.728022Z",
     "shell.execute_reply.started": "2021-08-03T19:37:12.703233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Model\n",
    "gold_model = LinearRegression()\n",
    "\n",
    "#Fit Model\n",
    "gold_model.fit(X, y)\n",
    "\n",
    "print(\"Making predicitons for the first 5 entries\\n\")\n",
    "print(X.head())\n",
    "print(\"\\nThe predictions are:\\n\")\n",
    "print(gold_model.predict(X.head()))\n",
    "print(\"\\nThe actual values are:\\n\")\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just based on the first five predictions. We can see that gold_model fitted to the whole data set is able to predict the adjusted close value within a dollar. We will run the whole model through validation and get its mean absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:12.731028Z",
     "iopub.status.busy": "2021-08-03T19:37:12.730586Z",
     "iopub.status.idle": "2021-08-03T19:37:12.742323Z",
     "shell.execute_reply": "2021-08-03T19:37:12.741358Z",
     "shell.execute_reply.started": "2021-08-03T19:37:12.730997Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_adj_close = gold_model.predict(X.head())\n",
    "print(mean_absolute_error(y.head(),predicted_adj_close))\n",
    "\n",
    "predicted_adj_close = gold_model.predict(X)\n",
    "print(mean_absolute_error(y, predicted_adj_close))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data\n",
    "Since we did not split up our data into train, test, and validation sets, the above model could be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:12.744246Z",
     "iopub.status.busy": "2021-08-03T19:37:12.74389Z",
     "iopub.status.idle": "2021-08-03T19:37:12.759134Z",
     "shell.execute_reply": "2021-08-03T19:37:12.758069Z",
     "shell.execute_reply.started": "2021-08-03T19:37:12.744196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Partition data into training and validation groups\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "# Define a new model for training set\n",
    "gold_model = LinearRegression()\n",
    "# Fit model\n",
    "gold_model.fit(train_X, train_y)\n",
    "\n",
    "#get predicted prices on validation data\n",
    "val_predictions = gold_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y,val_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:12.761066Z",
     "iopub.status.busy": "2021-08-03T19:37:12.76078Z",
     "iopub.status.idle": "2021-08-03T19:37:13.176787Z",
     "shell.execute_reply": "2021-08-03T19:37:13.175936Z",
     "shell.execute_reply.started": "2021-08-03T19:37:12.761039Z"
    }
   },
   "outputs": [],
   "source": [
    "gold_model = LinearRegression()\n",
    "\n",
    "# Bundle preporcessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('gold_model', gold_model)])\n",
    "# Preprocessing of training data, fit model\n",
    "my_pipeline.fit(train_X, train_y)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(val_X)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_score = mean_absolute_error(val_y, preds)\n",
    "print('MAE:', mae_score)\n",
    "\n",
    "# Display Model\n",
    "sns.regplot(x=val_y, y=preds, line_kws={\"color\":\"black\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model through Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:13.17872Z",
     "iopub.status.busy": "2021-08-03T19:37:13.178438Z",
     "iopub.status.idle": "2021-08-03T19:37:13.247973Z",
     "shell.execute_reply": "2021-08-03T19:37:13.246996Z",
     "shell.execute_reply.started": "2021-08-03T19:37:13.178681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=10,\n",
    "                              scoring = 'neg_mean_absolute_error')\n",
    "print(\"MAE scores:\\n\",scores,\"\\n\")\n",
    "print(\"Average MAE score (across all ten folds):\")\n",
    "print(scores.mean())\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(val_y,preds))\n",
    "print(\"\\nRMSE is\",rmse)\n",
    "\n",
    "r2 = r2_score(val_y, preds)\n",
    "print(\"\\nr2 score is\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:13.25033Z",
     "iopub.status.busy": "2021-08-03T19:37:13.249937Z",
     "iopub.status.idle": "2021-08-03T19:37:13.39356Z",
     "shell.execute_reply": "2021-08-03T19:37:13.392828Z",
     "shell.execute_reply.started": "2021-08-03T19:37:13.250279Z"
    }
   },
   "outputs": [],
   "source": [
    "my_model = XGBRegressor()\n",
    "my_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions using XGBoost model\n",
    "predictions = my_model.predict(val_X)\n",
    "print(\"Mean Absolute Error: \",mean_absolute_error(predictions, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much worse than our linear regression model. We will need to adjust some of our parameters to make sure we get the best result XGBoost can afford us. We will start by changing: \n",
    "1. The number of estimators - the number of times it will go through the modeling cycle.\n",
    "2. The early stopping round - the parameter we set to stop the model when our validation score stops improving.\n",
    "3. Learning rate - the parameter that means each model will help us less. The lower we set the learning rate, generally, the more accurate our predictions will be.\n",
    "4. n_jobs - we would change this to build our models faster. Ideally, you match this to the number of cores on your machine to shorten the amount of time the model is being fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:13.395286Z",
     "iopub.status.busy": "2021-08-03T19:37:13.394816Z",
     "iopub.status.idle": "2021-08-03T19:37:14.077265Z",
     "shell.execute_reply": "2021-08-03T19:37:14.076526Z",
     "shell.execute_reply.started": "2021-08-03T19:37:13.395251Z"
    }
   },
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000,\n",
    "                        learning_rate=0.03,\n",
    "                        n_jobs=4)\n",
    "my_model.fit(train_X, train_y,\n",
    "            early_stopping_rounds=5,\n",
    "            eval_set=[(val_X, val_y)],\n",
    "            verbose=False)\n",
    "\n",
    "predictions = my_model.predict(val_X)\n",
    "print(\"Mean Absolute Error\",\n",
    "      mean_absolute_error(predictions,val_y))\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(val_y,predictions))\n",
    "print(\"\\nRMSE is\", rmse)\n",
    "\n",
    "r2 = r2_score(val_y,predictions)\n",
    "print(\"\\nr2 score is\", r2)\n",
    "\n",
    "sns.regplot(x=val_y, y=predictions, line_kws={\"color\": \"black\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, XGBoost has a MAE of 0.325 and an RMSE of 0.490. The linear regression model gave an average MAE of 0.221 after it was run through ten folds of cross validation and an RMSE of 0.326. The linear regression model is slightly superior to XGBoost when given these parameters and validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "We've built a model that can predict gold's daily adjusted close value with good accuracy. However, this dataset came with 79 predictor features. Obviously, the predictors I just used are most closely related to the adjusted close value. Now we will use feature engineering to determine what are highly correlated features and see if we can build a better model with them. We'll start by ranking features with **mutual information** and show which features rank the highest with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:14.07869Z",
     "iopub.status.busy": "2021-08-03T19:37:14.078328Z",
     "iopub.status.idle": "2021-08-03T19:37:14.105998Z",
     "shell.execute_reply": "2021-08-03T19:37:14.105201Z",
     "shell.execute_reply.started": "2021-08-03T19:37:14.078664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Refresh on what all of the features look like\n",
    "# There are 79 predictor columns. I am not including Adj Close and Close of the 81 total.\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:14.108453Z",
     "iopub.status.busy": "2021-08-03T19:37:14.107996Z",
     "iopub.status.idle": "2021-08-03T19:37:14.129332Z",
     "shell.execute_reply": "2021-08-03T19:37:14.12808Z",
     "shell.execute_reply.started": "2021-08-03T19:37:14.108352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create new ds with all predictor features. Take Adj Close as Y\n",
    "# Remove Close because it is too close to Adj Close\n",
    "X = df.copy()\n",
    "y = X.pop('Adj Close')\n",
    "date = X.pop('Date')\n",
    "X.pop('Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:14.131876Z",
     "iopub.status.busy": "2021-08-03T19:37:14.131425Z",
     "iopub.status.idle": "2021-08-03T19:37:16.215495Z",
     "shell.execute_reply": "2021-08-03T19:37:16.214682Z",
     "shell.execute_reply.started": "2021-08-03T19:37:14.131844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create mutual info scores\n",
    "\n",
    "def make_mi_scores (X, y):\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a bar plot to show each feature's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:16.217068Z",
     "iopub.status.busy": "2021-08-03T19:37:16.216739Z",
     "iopub.status.idle": "2021-08-03T19:37:17.778087Z",
     "shell.execute_reply": "2021-08-03T19:37:17.777314Z",
     "shell.execute_reply.started": "2021-08-03T19:37:16.217039Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "    \n",
    "plt.figure(dpi=100, figsize=(10,18))\n",
    "plot_mi_scores(mi_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No wonder our earlier models performed so well. We were already using the best predictors: High, Low, and Open. The predictor we used that has very low mutual information is Volume. In fact, Volume has a lower mutual information than other volumes for stock market indicies. Thanks to the graph above, we know to drop Volume as a predictor and perhaps add in higher ranking predictors such as GDX_High and GDX_Close. It seems that the volume and trend categories of predictors always have a lower MI score than any high, low, or open predictor.\n",
    "\n",
    "High, Low and Open are extremely correlated to the Adj Close value. Let's create simple regplots to magnify their relationships. You will see that as their mutual information rank gets lower, the less correlated that feature's regplot is with adjusted close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:17.779495Z",
     "iopub.status.busy": "2021-08-03T19:37:17.779118Z",
     "iopub.status.idle": "2021-08-03T19:37:18.243634Z",
     "shell.execute_reply": "2021-08-03T19:37:18.242571Z",
     "shell.execute_reply.started": "2021-08-03T19:37:17.779466Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_high = sns.regplot(x=\"High\", y=\"Adj Close\", data=df, line_kws={\"color\": \"black\"}).set(title=\"Gold's Daily High\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:18.245641Z",
     "iopub.status.busy": "2021-08-03T19:37:18.245228Z",
     "iopub.status.idle": "2021-08-03T19:37:18.706015Z",
     "shell.execute_reply": "2021-08-03T19:37:18.705021Z",
     "shell.execute_reply.started": "2021-08-03T19:37:18.245595Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_low = sns.regplot(x=\"Low\", y=\"Adj Close\", data=df, line_kws={\"color\": \"black\"}).set(title=\"Gold's Daily Low\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:18.709767Z",
     "iopub.status.busy": "2021-08-03T19:37:18.709385Z",
     "iopub.status.idle": "2021-08-03T19:37:19.160906Z",
     "shell.execute_reply": "2021-08-03T19:37:19.159894Z",
     "shell.execute_reply.started": "2021-08-03T19:37:18.709684Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_close = sns.regplot(x=\"Open\", y=\"Adj Close\", data=df, line_kws={\"color\": \"black\"}).set(title=\"Gold's Daily Open\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these three native features are very good predictors for the adjusted close price. However, what if we can create a new feature that can lend a little more insight into the price of gold. In the code below, we will create a feature called \"daily change\" by taking the price difference from open and close. Daily change is useless as a predictor on its own, but maybe we can apply it to time-series data to see any trends. Daily change is a way to track the volatility of gold prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:19.162471Z",
     "iopub.status.busy": "2021-08-03T19:37:19.16217Z",
     "iopub.status.idle": "2021-08-03T19:37:19.604002Z",
     "shell.execute_reply": "2021-08-03T19:37:19.602962Z",
     "shell.execute_reply.started": "2021-08-03T19:37:19.162441Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Daily_Change\"] = abs(X.High - X.Low)\n",
    "\n",
    "# Convert Date from string to datetime to give us yearly ticks on the X-axis\n",
    "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')\n",
    "\n",
    "# Plot volatility\n",
    "sns.set(rc={\"figure.figsize\":(20, 4)})\n",
    "daily_change = sns.lineplot(x=\"Date\", y=\"Daily_Change\", data=df).set(title=\"Gold's Daily Change/Volatility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that gold prices have gotten less volatile since 2012. Around after mid-2017, gold seems to have become less volatile to the end of 2018. If we were to split up the data from 2012-2013 and 2017-2018 and run them through our linear regression model, we would get better performance from the 2017-2018 dataset because the prices have less daily variation than the 2012-2013 subset. The main takeaway here is that gold volatility has decreased overtime. However, the Adjusted Close has also gone down, as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:19.606065Z",
     "iopub.status.busy": "2021-08-03T19:37:19.60564Z",
     "iopub.status.idle": "2021-08-03T19:37:20.047665Z",
     "shell.execute_reply": "2021-08-03T19:37:20.046549Z",
     "shell.execute_reply.started": "2021-08-03T19:37:19.60602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjusted Close with Time Series\n",
    "sns.set(rc={\"figure.figsize\":(20, 4)})\n",
    "daily_change = sns.lineplot(x=\"Date\", y=\"Adj Close\", data=df).set(title=\"Gold's Adjusted Daily Close Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll apply principal component analysis (PCA). PCA will be good to use with this dataset for two reasons. \n",
    "* The first is that PCA works well with numeric features. All of our features, with the exception of Date, are numeric features.\n",
    "* Many of our features are redundant and are closely related mutual information scores. A lot of them can be removed or combined to create principal components.\n",
    "We already know that High, Low, and Open have the highest mutual information scores. We also know that those features alone produce a high-performing linear regression model. To experiment with something new. We will leave in the top six features, which means including the gold index(GDX) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:20.049255Z",
     "iopub.status.busy": "2021-08-03T19:37:20.048972Z",
     "iopub.status.idle": "2021-08-03T19:37:20.063062Z",
     "shell.execute_reply": "2021-08-03T19:37:20.061942Z",
     "shell.execute_reply.started": "2021-08-03T19:37:20.049228Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\"High\", \"Low\", \"Open\", \"GDX_High\", \"GDX_Low\", \"GDX_Close\"]\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('Adj Close')\n",
    "date = X.pop('Date')\n",
    "X.pop('Close')\n",
    "X = X.loc[:, features]\n",
    "\n",
    "# Standardize the new df. PCA is sensitive to scale.\n",
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:20.064976Z",
     "iopub.status.busy": "2021-08-03T19:37:20.064531Z",
     "iopub.status.idle": "2021-08-03T19:37:20.088692Z",
     "shell.execute_reply": "2021-08-03T19:37:20.087602Z",
     "shell.execute_reply.started": "2021-08-03T19:37:20.064931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create principal componenets\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range (X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:20.090663Z",
     "iopub.status.busy": "2021-08-03T19:37:20.090117Z",
     "iopub.status.idle": "2021-08-03T19:37:20.106751Z",
     "shell.execute_reply": "2021-08-03T19:37:20.105573Z",
     "shell.execute_reply.started": "2021-08-03T19:37:20.09062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wrap the PCA loadings up in a dataframe\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,       # Transpose the matrix of loadings\n",
    "    columns=component_names, # to turn columns into principal components\n",
    "    index = X.columns,       # and the rows are original features, so we can identify them\n",
    ")\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:20.108611Z",
     "iopub.status.busy": "2021-08-03T19:37:20.108192Z",
     "iopub.status.idle": "2021-08-03T19:37:20.47685Z",
     "shell.execute_reply": "2021-08-03T19:37:20.475853Z",
     "shell.execute_reply.started": "2021-08-03T19:37:20.108569Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    \n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0,cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"%Cumulative Variance\", ylim=(0.0,1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "# Look at the explained variance from PCA\n",
    "plot_variance(pca);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see principal component 1 (PC1) is the most informative by far. A disparity is expected usually between PC1 and the remainders. However, this is a very high disparity. If we didn't look at our features through mutual information earlier. This would raise concern. However, we already know that the first three features are highly correlated with our target feature of adjusted close, so these results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:20.480745Z",
     "iopub.status.busy": "2021-08-03T19:37:20.480352Z",
     "iopub.status.idle": "2021-08-03T19:37:20.654129Z",
     "shell.execute_reply": "2021-08-03T19:37:20.653161Z",
     "shell.execute_reply.started": "2021-08-03T19:37:20.480694Z"
    }
   },
   "outputs": [],
   "source": [
    "# View MI Scores for the principal components\n",
    "mi_scores = make_mi_scores(X_pca, y)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data Pipeline with PCA\n",
    "Now that we can see how worthy each principal component is, we can better determine which ones we should add to our new pipeline. In this instance, we will include all prinicpal components in our new pipeline The goal is to beat an MAE of 0.221 after a 10-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:20.655802Z",
     "iopub.status.busy": "2021-08-03T19:37:20.655495Z",
     "iopub.status.idle": "2021-08-03T19:37:21.060156Z",
     "shell.execute_reply": "2021-08-03T19:37:21.059099Z",
     "shell.execute_reply.started": "2021-08-03T19:37:20.655773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Partition the PCA dataframe into training and validation groups\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_pca, y, random_state = 0)\n",
    "\n",
    "gold_model = LinearRegression()\n",
    "\n",
    "# Bundle preporcessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('gold_model', gold_model)])\n",
    "# Preprocessing of training data, fit model\n",
    "my_pipeline.fit(train_X, train_y)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(val_X)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_score = mean_absolute_error(val_y, preds)\n",
    "print('MAE:', mae_score)\n",
    "\n",
    "# Display Model\n",
    "sns.set(rc={\"figure.figsize\":(6,6)})\n",
    "sns.regplot(x=val_y, y=preds, line_kws={\"color\":\"black\"}).set(title=\"Linear Regression with PCA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see PCA helped us improve the model's MAE by 0.20. Let's run the new pipeline through cross-validation to get a more accurate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:21.062309Z",
     "iopub.status.busy": "2021-08-03T19:37:21.061863Z",
     "iopub.status.idle": "2021-08-03T19:37:21.129571Z",
     "shell.execute_reply": "2021-08-03T19:37:21.128787Z",
     "shell.execute_reply.started": "2021-08-03T19:37:21.062258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X_pca, y,\n",
    "                              cv=10,\n",
    "                              scoring = 'neg_mean_absolute_error')\n",
    "print(\"MAE scores:\\n\",scores,\"\\n\")\n",
    "print(\"Average MAE score (across all ten folds):\")\n",
    "print(scores.mean())\n",
    "rmse = math.sqrt(mean_squared_error(val_y,preds))\n",
    "print(\"\\nRMSE is\", rmse)\n",
    "r2 = r2_score(val_y,preds)\n",
    "print(\"\\nr2 score is\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Three Models Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T19:37:21.13122Z",
     "iopub.status.busy": "2021-08-03T19:37:21.130832Z",
     "iopub.status.idle": "2021-08-03T19:37:21.14793Z",
     "shell.execute_reply": "2021-08-03T19:37:21.146955Z",
     "shell.execute_reply.started": "2021-08-03T19:37:21.131179Z"
    }
   },
   "outputs": [],
   "source": [
    "results = [['Linear Regression', 0.221, 0.326, 0.999672],\n",
    "           ['Gradient Boosting (XGBoost)', 0.325, 0.490, 0.999259],\n",
    "           ['Linear Regression with PCA', 0.193, 0.275, 0.999766]]\n",
    "results_df = pd.DataFrame(results, columns = ['Model Type', 'MAE', 'RMSE', 'r2'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good news!** Running the PCA model through ten-fold cross validation gave us our best model performance across all metrics. The simple linear regression model came in second on all metrics as well. XGBoost came in last this time. Although there is slight variation in performance, all three of these models perform well because the top three features are so highly correlated with adjusted time. The dataset has many features, but it would not improve the model significantly by adding ones beyond the top six to a regression model.\n",
    "\n",
    "Thanks for your time! \n",
    "\n",
    "All feedback is welcome!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
